{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ignored-nevada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, gzip, json, pickle, shutil, random\n",
    "sys.path.append('../app')\n",
    "from config import Config\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "#from fse.models import SIF\n",
    "#from fse import IndexedList\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config IPCompleter.greedy=True\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "helpful-incidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "grave-salad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 1000000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels = ['Commercial', 'Fearmonger', 'HashtagGamer', 'LeftTroll', \n",
    "                'NewsFeed', 'NormalUser', 'RightTroll', 'Unknown']\n",
    "\n",
    "with gzip.open(config.feature_x_path, 'rb') as fz:\n",
    "    feats = pickle.load(fz)\n",
    "\n",
    "#string labels for all 8 types\n",
    "type_labels = [feat['type'] for feat in feats]\n",
    "type_lab2idx = {c:i for i,c in enumerate(class_labels)}\n",
    "\n",
    "#string binary labels\n",
    "bin_labels = [l if l=='NormalUser' else 'TrollUser' for l in type_labels]\n",
    "bin_lab2idx = {'NormalUser':0, 'TrollUser':1}\n",
    "\n",
    "with open(config.feature_y_path, 'rb') as f:\n",
    "    #int binary labels\n",
    "    bin_y = pickle.load(f)\n",
    "    \n",
    "#int labels for all 8 types\n",
    "type_y = [lab2idx[l] for l in type_labels]     \n",
    "\n",
    "len(feats), len(bin_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "opponent-christmas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(765127, 765127, 765127)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a subset of data consisting of only NormalUser, LeftTroll, RightTroll\n",
    "three_class_lab2int = {'LeftTroll':0, 'NormalUser':1, 'RightTroll':2}\n",
    "\n",
    "three_class_feats = [f for f in feats if f['type'] in three_class_lab2int]\n",
    "three_class_labels = [f['type'] for f in three_class_feats]\n",
    "three_class_y = [three_class_lab2int[l] for l in three_class_labels]\n",
    "\n",
    "len(three_class_feats), len(three_class_labels), len(three_class_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "copyrighted-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmas = [f['lemmas'].strip() for f in feats]\n",
    "tokens = [f['tokens'].strip() for f in feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "changed-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_w2v(texts, model_path, vec_dim=128, window=5):\n",
    "    model = Word2Vec(sentences=texts, size=vec_dim, window=window, min_count=10, workers=4)\n",
    "    model.train(texts, total_examples=len(texts), epochs=5)\n",
    "\n",
    "    print('...Storing trained word2vec model')\n",
    "    model.save(model_path)\n",
    "    \n",
    "    for word in ['one', 'cat', 'fun', 'stupid', 'lol']:\n",
    "        print('close to %s' % word)\n",
    "        for w,s in model.wv.most_similar(word):\n",
    "            print('   %s, %.6f' % (w,s))\n",
    "            \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_ft(texts, model_path, vec_dim=128, window=5):\n",
    "    model = FastText(sentences=texts, size=vec_dim, window=window, min_count=10, workers=4)\n",
    "    model.train(texts, total_examples=len(texts), epochs=5)\n",
    "\n",
    "    print('...Storing trained fasttext model')\n",
    "    model.save(model_path)\n",
    "    \n",
    "    for word in ['one', 'cat', 'fun', 'stupid', 'lol']:\n",
    "        print('close to %s' % word)\n",
    "        for w,s in model.wv.most_similar(word):\n",
    "            print('   %s, %.6f' % (w,s))\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "identified-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_texts = [[t for t in l.split(' ') if t and not t[0]=='#'] for l in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "danish-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "proof-extra",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Storing trained word2vec model\n",
      "close to one\n",
      "   all, 0.531845\n",
      "   that, 0.526677\n",
      "   One, 0.522539\n",
      "   the, 0.522073\n",
      "   probably, 0.515429\n",
      "   every, 0.515309\n",
      "   actually, 0.510083\n",
      "   rest, 0.507020\n",
      "   ever, 0.506661\n",
      "   earth, 0.505416\n",
      "close to cat\n",
      "   dog, 0.798502\n",
      "   kid, 0.701612\n",
      "   girl, 0.691865\n",
      "   mom, 0.682723\n",
      "   puppy, 0.677790\n",
      "   pig, 0.670335\n",
      "   baby, 0.662166\n",
      "   neighbor, 0.658788\n",
      "   pet, 0.658629\n",
      "   stomach, 0.653281\n",
      "close to fun\n",
      "   great, 0.701722\n",
      "   nice, 0.682614\n",
      "   awesome, 0.640763\n",
      "   wonderful, 0.639966\n",
      "   lovely, 0.639589\n",
      "   good, 0.633600\n",
      "   yummy, 0.632005\n",
      "   fantastic, 0.620747\n",
      "   cool, 0.607066\n",
      "   fabulous, 0.586966\n",
      "close to stupid\n",
      "   dumb, 0.843443\n",
      "   nasty, 0.705477\n",
      "   annoying, 0.698032\n",
      "   silly, 0.695454\n",
      "   ridiculous, 0.679609\n",
      "   weird, 0.668598\n",
      "   lame, 0.661439\n",
      "   crazy, 0.659034\n",
      "   funny, 0.655009\n",
      "   boring, 0.652486\n",
      "close to lol\n",
      "   haha, 0.871714\n",
      "   lmao, 0.867576\n",
      "   hahaha, 0.792742\n",
      "   smh, 0.756514\n",
      "   tho, 0.754352\n",
      "   Lol, 0.753319\n",
      "   :p, 0.722850\n",
      "   lmfao, 0.712313\n",
      "   ;), 0.707175\n",
      "   omg, 0.705730\n"
     ]
    }
   ],
   "source": [
    "w2v_token_128_model = train_w2v(token_texts, model_path='w2v_token_%d.model' % word_vec_dim, \n",
    "                                vec_dim=word_vec_dim, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "diverse-storage",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Storing trained fasttext model\n",
      "close to one\n",
      "   fone, 0.808282\n",
      "   none, 0.766799\n",
      "   lone, 0.744887\n",
      "   throne, 0.740066\n",
      "   tone, 0.737139\n",
      "   cone, 0.736137\n",
      "   noone, 0.731479\n",
      "   ozone, 0.728656\n",
      "   Ozone, 0.728233\n",
      "   clone, 0.722626\n",
      "close to cat\n",
      "   dog, 0.830231\n",
      "   goat, 0.806488\n",
      "   belly, 0.778014\n",
      "   beard, 0.777001\n",
      "   doggy, 0.770091\n",
      "   doggie, 0.769193\n",
      "   mat, 0.766632\n",
      "   bone, 0.765334\n",
      "   pet, 0.765020\n",
      "   catfish, 0.763890\n",
      "close to fun\n",
      "   great, 0.712751\n",
      "   awesome, 0.702640\n",
      "   funky, 0.699654\n",
      "   funn, 0.685181\n",
      "   fabulous, 0.676436\n",
      "   wonderful, 0.670345\n",
      "   funk, 0.663538\n",
      "   nice, 0.663520\n",
      "   gruesome, 0.660641\n",
      "   good, 0.657015\n",
      "close to stupid\n",
      "   stupidly, 0.878712\n",
      "   stupidity, 0.792253\n",
      "   annoying, 0.788235\n",
      "   disgusting, 0.767651\n",
      "   pathetic, 0.751139\n",
      "   silly, 0.749733\n",
      "   fucking, 0.743266\n",
      "   fugly, 0.742206\n",
      "   weirdo, 0.741919\n",
      "   ridiculous, 0.741697\n",
      "close to lol\n",
      "   lolol, 0.938090\n",
      "   haha, 0.892991\n",
      "   hahaha, 0.875494\n",
      "   hahahha, 0.854620\n",
      "   hahahaha, 0.848657\n",
      "   hahah, 0.842310\n",
      "   lolz, 0.841708\n",
      "   lolzz, 0.841167\n",
      "   bahaha, 0.834871\n",
      "   hahahah, 0.833534\n"
     ]
    }
   ],
   "source": [
    "ft_token_128_model = train_ft(token_texts, model_path='ft_token_%d.model' % word_vec_dim, \n",
    "                              vec_dim=word_vec_dim, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "patient-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, hid1_size, hid2_size, out_size, text_vocab_size, text_emb_dim, \n",
    "                 pos_vocab_size, pos_emb_dim, ent_vocab_size, ent_emb_dim, \n",
    "                 hashtag_vocab_size, hashtag_emb_dim, in_feat_dim, out_feat_dim):\n",
    "        super().__init__()\n",
    "        self.text_embedder = nn.EmbeddingBag(text_vocab_size, text_emb_dim)\n",
    "        self.pos_embedder = nn.EmbeddingBag(pos_vocab_size, pos_emb_dim)\n",
    "        self.ent_embedder = nn.EmbeddingBag(ent_vocab_size, ent_emb_dim)\n",
    "        self.hashtag_embedder = nn.EmbeddingBag(hashtag_vocab_size, hashtag_emb_dim)\n",
    "        self.feat_transform = nn.Linear(in_feat_dim, out_feat_dim)\n",
    "        \n",
    "        self.hid_layer1 = nn.Linear(text_emb_dim + pos_emb_dim + ent_emb_dim + hashtag_emb_dim + out_feat_dim, hid1_size)\n",
    "        self.hid_layer2 = nn.Linear(hid1_size, hid2_size)\n",
    "        self.out_layer = nn.Linear(hid2_size, out_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hid1_size)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hid2_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x_text, x_pos, x_ent, x_hashtag, x_feats):\n",
    "        text_emb = self.text_embedder(x_text)\n",
    "        pos_emb = self.pos_embedder(x_pos)\n",
    "        ent_emb = self.ent_embedder(x_ent)\n",
    "        hashtag_emb = self.hashtag_embedder(x_hashtag)\n",
    "        feat_trans = self.feat_transform(x_feats)\n",
    "        \n",
    "        x = torch.cat((text_emb, pos_emb, ent_emb, hashtag_emb, feat_trans), dim=1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.hid_layer1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        \n",
    "        x = self.relu(self.hid_layer2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        \n",
    "        x = self.out_layer(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "geographic-iceland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature2idx_dicts(feats, word_model):\n",
    "    word2idx = {}\n",
    "    for i in range(len(word_model.wv.vocab)):\n",
    "        word2idx[word_model.wv.index2word[i]] = i\n",
    "\n",
    "    pos2idx = {}\n",
    "    i=1\n",
    "    for feat in feats:\n",
    "        for pos in feat['pos'].split(' '):\n",
    "            if not pos:\n",
    "                continue\n",
    "            if pos not in pos2idx:\n",
    "                pos2idx[pos] = i\n",
    "                i+=1\n",
    "    pos2idx['<PAD>'] = 0\n",
    "\n",
    "    ent2idx = {}\n",
    "    i=1\n",
    "    for feat in feats:\n",
    "        for et in feat['ent_types']:\n",
    "            #skip some bogus entity types\n",
    "            if not et or not et.isupper():\n",
    "                continue\n",
    "            if et not in ent2idx:\n",
    "                ent2idx[et] = i\n",
    "                i+=1\n",
    "    ent2idx['<PAD>'] = 0\n",
    "\n",
    "    #only use hashtags with over 10 occurrences\n",
    "    hashtags = []\n",
    "    for feat in feats:\n",
    "        hashtags.extend(feat['hashtags'])\n",
    "\n",
    "    hashtag_cts = Counter(hashtags)\n",
    "    hashtag_cts = {h:c for h,c in hashtag_cts.items() if c>=10}\n",
    "\n",
    "    hashtag2idx = {h:i+1 for i,h in enumerate(hashtag_cts.keys())}\n",
    "    hashtag2idx['<PAD>'] = 0\n",
    "\n",
    "    feat_fields = ['emoji_ratio', 'link_ratio', 'user_ratio', 'oov_ratio']\n",
    "    \n",
    "    return word2idx, pos2idx, ent2idx, hashtag2idx, feat_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "placed-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx, pos2idx, ent2idx, hashtag2idx, feat_fields = get_feature2idx_dicts(feats, w2v_token_128_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "placed-wayne",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(feats, word2idx, pos2idx, ent2idx, hashtag2idx, feat_fields, lab2idx,\n",
    "               text_seq_len = 30, ent_seq_len = 2, hashtag_seq_len = 2, \n",
    "                use_lemmas=False, reload=False):\n",
    "    if reload and os.path.exists('y_type_%d.pt' % len(lab2idx)):\n",
    "        x_text = torch.load('x_text.pt')\n",
    "        x_pos = torch.load('x_pos.pt')\n",
    "        x_ent = torch.load('x_ent.pt')\n",
    "        x_hashtag = torch.load('x_hashtag.pt')\n",
    "        x_feats = torch.load('x_feats.pt')\n",
    "        y_bin = torch.load('y_bin.pt')\n",
    "        y_type = torch.load('y_type_%d.pt' % len(lab2idx))\n",
    "        \n",
    "        return x_text, x_pos, x_ent, x_hashtag, x_feats, y_bin, y_type\n",
    "    \n",
    "    x_text = []\n",
    "    x_pos = []\n",
    "    x_ent = []\n",
    "    x_hashtag = []\n",
    "    x_feats = []\n",
    "    y_bin = []\n",
    "    y_type = []\n",
    "    \n",
    "    for feat in feats:\n",
    "        #if feat['type'] not in lab2idx:\n",
    "        #    continue\n",
    "\n",
    "        if use_lemmas:\n",
    "            #we use the actual word 'pad' as the pad token bc the pretrained wts must be the same size as the vocab\n",
    "            s = [word2idx[t] for t in feat['lemmas'].split(' ') if t and t in word2idx] + [word2idx['pad']]*text_seq_len\n",
    "        else:\n",
    "            s = [word2idx[t] for t in feat['tokens'].split(' ') if t and t in word2idx] + [word2idx['pad']]*text_seq_len\n",
    "        x_text.append(torch.tensor(s[:text_seq_len]))\n",
    "\n",
    "        s = [pos2idx[t] for t in feat['pos'].split(' ') if t and t in pos2idx] + [pos2idx['<PAD>']]*text_seq_len\n",
    "        x_pos.append(torch.tensor(s[:text_seq_len]))\n",
    "\n",
    "        s = [ent2idx[t] for t in feat['ent_types'] if t and t in ent2idx] + [ent2idx['<PAD>']]*ent_seq_len\n",
    "        x_ent.append(torch.tensor(s[:ent_seq_len]))\n",
    "\n",
    "        s = [hashtag2idx[t] for t in feat['hashtags'] if t and t in hashtag2idx] + [hashtag2idx['<PAD>']]*hashtag_seq_len\n",
    "        x_hashtag.append(torch.tensor(s[:hashtag_seq_len]))\n",
    "\n",
    "        x_feats.append(torch.tensor([feat[f] for f in feat_fields]))\n",
    "\n",
    "        y_bin.append(0 if feat['type']=='NormalUser' else 1)\n",
    "        y_type.append(lab2idx[feat['type']])\n",
    "\n",
    "    x_text = torch.vstack(x_text)\n",
    "    x_pos = torch.vstack(x_pos)\n",
    "    x_ent = torch.vstack(x_ent)\n",
    "    x_hashtag = torch.vstack(x_hashtag)\n",
    "    x_feats = torch.vstack(x_feats)\n",
    "    y_bin = torch.tensor(y_bin)\n",
    "    y_type = torch.tensor(y_type)\n",
    "\n",
    "    torch.save(x_text, 'x_text.pt')\n",
    "    torch.save(x_pos, 'x_pos.pt')\n",
    "    torch.save(x_ent, 'x_ent.pt')\n",
    "    torch.save(x_hashtag, 'x_hashtag.pt')\n",
    "    torch.save(x_feats, 'x_feats.pt')\n",
    "    torch.save(y_bin, 'y_bin.pt')\n",
    "    torch.save(y_type, 'y_type_%d.pt' % len(lab2idx))\n",
    "    \n",
    "    return x_text, x_pos, x_ent, x_hashtag, x_feats, y_bin, y_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "interim-ivory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000000, 30]),\n",
       " torch.Size([1000000, 30]),\n",
       " torch.Size([1000000, 2]),\n",
       " torch.Size([1000000, 2]),\n",
       " torch.Size([1000000, 4]),\n",
       " torch.Size([1000000]),\n",
       " tensor([   22,     7,     2,   233,     5,   111, 24489,   109,     5,   201,\n",
       "         24489,    36,   494,   797,     0, 12496, 12496, 12496, 12496, 12496,\n",
       "         12496, 12496, 12496, 12496, 12496, 12496, 12496, 12496, 12496, 12496]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_text, x_pos, x_ent, x_hashtag, x_feats, y_bin, y_type = get_tensors(feats, word2idx, pos2idx, ent2idx, hashtag2idx, \n",
    "                                        feat_fields, type_lab2idx, use_lemmas=False, reload=False)\n",
    "x_text.size(), x_pos.size(), x_ent.size(), x_hashtag.size(), x_feats.size(), y_bin.size(), x_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "another-theorem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 500000, 1: 500000})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_bin.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "chinese-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_text, x_pos, x_ent, x_hashtag, x_feats, y, class_labels, model, \n",
    "                optimizer, scheduler, batch_size, epochs, device, test_size=10000):\n",
    "    x_text_test = x_text[:test_size]\n",
    "    x_text_cv = x_text[test_size:2*test_size]\n",
    "    x_text_train = x_text[2*test_size:]\n",
    "\n",
    "    x_pos_test = x_pos[:test_size]\n",
    "    x_pos_cv = x_pos[test_size:2*test_size]\n",
    "    x_pos_train = x_pos[2*test_size:]\n",
    "\n",
    "    x_ent_test = x_ent[:test_size]\n",
    "    x_ent_cv = x_ent[test_size:2*test_size]\n",
    "    x_ent_train = x_ent[2*test_size:]\n",
    "\n",
    "    x_hashtag_test = x_hashtag[:test_size]\n",
    "    x_hashtag_cv = x_hashtag[test_size:2*test_size]\n",
    "    x_hashtag_train = x_hashtag[2*test_size:]\n",
    "\n",
    "    x_feats_test = x_feats[:test_size]\n",
    "    x_feats_cv = x_feats[test_size:2*test_size]\n",
    "    x_feats_train = x_feats[2*test_size:]\n",
    "\n",
    "    y_test = y[:test_size]\n",
    "    y_cv = y[test_size:2*test_size]\n",
    "    y_train = y[2*test_size:]\n",
    "    \n",
    "    x_text_train_batches = [x_text_train[i*batch_size:(i+1)*batch_size] for i in range((x_text_train.shape[0]//batch_size) + 1)]\n",
    "    x_pos_train_batches = [x_pos_train[i*batch_size:(i+1)*batch_size] for i in range((x_pos_train.shape[0]//batch_size) + 1)]\n",
    "    x_ent_train_batches = [x_ent_train[i*batch_size:(i+1)*batch_size] for i in range((x_ent_train.shape[0]//batch_size) + 1)]\n",
    "    x_hashtag_train_batches = [x_hashtag_train[i*batch_size:(i+1)*batch_size] for i in range((x_hashtag_train.shape[0]//batch_size) + 1)]\n",
    "    x_feats_train_batches = [x_feats_train[i*batch_size:(i+1)*batch_size] for i in range((x_feats_train.shape[0]//batch_size) + 1)]\n",
    "    y_train_batches = [y_train[i*batch_size:(i+1)*batch_size] for i in range((y_train.shape[0]//batch_size) + 1)]\n",
    "    \n",
    "    best_cv_acc = 0\n",
    "    losses = []\n",
    "    no_improvement = 0\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        \n",
    "        idx = list(range(len(x_text_train_batches)))\n",
    "        #random.shuffle(idx)\n",
    "        #x_batches = [x_batches[i] for i in idx]\n",
    "        #y_batches = [y_batches[i] for i in idx]\n",
    "        \n",
    "        for i in range(len(x_text_train_batches)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(x_text_train_batches[i].to(device),\n",
    "                           x_pos_train_batches[i].to(device), \n",
    "                           x_ent_train_batches[i].to(device),\n",
    "                           x_hashtag_train_batches[i].to(device), \n",
    "                           x_feats_train_batches[i].to(device))\n",
    "\n",
    "            #y_pred = torch.argmax(y_pred, dim=1).float().unsqueeze(1)\n",
    "            #loss = criterion(y_pred, y_train_batches[i])#)\n",
    "            loss = F.nll_loss(y_pred, y_train_batches[i])\n",
    "            acc = binary_acc(y_pred, y_train_batches[i].float())#.unsqueeze(1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "            if i and i%1000==0:\n",
    "                print('  ...Batch %d\\tEpoch %d\\tLoss: %.8f\\tTrain Acc: %.8f' % (\n",
    "                    i, epoch, epoch_loss/i, epoch_acc/i))\n",
    "                losses.append(epoch_loss/i)\n",
    "        \n",
    "        cv_acc = test(x_text_cv, x_pos_cv, x_ent_cv, x_hashtag_cv, x_feats_cv, y_cv, \n",
    "                      model, class_labels)\n",
    "        if cv_acc>best_cv_acc:\n",
    "            print('\\n *** CV Accuracy Improved: %.4f -> %.4f ***' % (best_cv_acc, cv_acc))\n",
    "            torch.save(model.state_dict(), 'lin_model_bin.pt')\n",
    "            best_cv_acc=cv_acc\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "        \n",
    "        if no_improvement >= 5:\n",
    "            break\n",
    "            \n",
    "        scheduler.step(cv_acc)\n",
    "            \n",
    "        print('\\nEpoch %d\\tLoss: %.8f\\tTrain Acc: %.8f\\tCV Acc: %.8f\\tNo Improvement: %d' % (\n",
    "            epoch, epoch_loss/len(x_text_train_batches), epoch_acc/len(x_text_train_batches), cv_acc, no_improvement))\n",
    "        \n",
    "    test_acc = test(x_text_test, x_pos_test, x_ent_test, x_hashtag_test, x_feats_test, y_test, \n",
    "                    model, class_labels, print_confusion=True)\n",
    "    print('\\n\\n\\nFinal Test Acc: %.8f' % (test_acc))\n",
    "        \n",
    "    return model, losses\n",
    "        \n",
    "        \n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred = torch.argmax(y_pred, dim=1).cpu()\n",
    "    cor = (y_pred == y_test).int().sum()\n",
    "    acc = cor/y_test.size()[0]\n",
    "    \n",
    "    return acc\n",
    "\n",
    "\n",
    "def test(x_text_test, x_pos_test, x_ent_test, x_hashtag_test, x_feats_test, y_test, \n",
    "         model, class_labels, print_confusion=False):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x_text_test, x_pos_test, x_ent_test, x_hashtag_test, x_feats_test)\n",
    "        y_pred = torch.argmax(y_pred, dim=1).cpu()\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    print(classification_report(y_test.tolist(), y_pred.tolist(), target_names=class_labels, digits=4))\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    if print_confusion:\n",
    "        cm = confusion_matrix(y_test.tolist(), y_pred.tolist())\n",
    "        f = sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_labels, yticklabels=class_labels)\n",
    "\n",
    "    cor = (y_pred == y_test).int().sum()\n",
    "    acc = cor/y_test.size()[0]\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "occupational-governor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5828, -0.1913, -0.5416,  ..., -0.6504,  0.8422, -1.2246],\n",
       "        [ 1.5625,  1.6683,  1.1405,  ...,  1.2371, -0.1109,  0.2551],\n",
       "        [ 1.3750,  0.9242, -0.5991,  ...,  0.4427,  0.4270,  1.0353],\n",
       "        ...,\n",
       "        [ 0.1085, -0.1361, -0.1510,  ..., -0.2159,  0.0067, -0.0874],\n",
       "        [-0.0861,  0.0119, -0.0292,  ..., -0.0984,  0.1513,  0.0046],\n",
       "        [ 0.2633,  0.1682, -0.3559,  ..., -0.4158, -0.0575,  0.0583]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 class instance, using word2vec embeddings\n",
    "\n",
    "lin_model = LinearClassifier(hid1_size=512, hid2_size=128, out_size=2, \n",
    "                             text_vocab_size=len(word2idx), text_emb_dim=word_vec_dim, \n",
    "                             pos_vocab_size=len(pos2idx), pos_emb_dim=8,\n",
    "                             ent_vocab_size=len(ent2idx), ent_emb_dim=8, \n",
    "                             hashtag_vocab_size=len(hashtag2idx), hashtag_emb_dim=64, \n",
    "                             in_feat_dim=len(feat_fields), out_feat_dim=len(feat_fields))\n",
    "\n",
    "#copy pretrained word weights\n",
    "lin_model.text_embedder.weight.data.copy_(torch.FloatTensor(w2v_token_128_model.wv.vectors))\n",
    "#lin_model.text_embedder.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fewer-impact",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "lin_model.to(device)\n",
    "lin_model.train()\n",
    "\n",
    "optimizer = optim.Adam(lin_model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "knowing-input",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ...Batch 1000\tEpoch 1\tLoss: 0.21350766\tTrain Acc: 0.91270312\n",
      "  ...Batch 2000\tEpoch 1\tLoss: 0.16862020\tTrain Acc: 0.93171875\n",
      "  ...Batch 3000\tEpoch 1\tLoss: 0.14653585\tTrain Acc: 0.94063021\n",
      "  ...Batch 4000\tEpoch 1\tLoss: 0.13249760\tTrain Acc: 0.94639648\n",
      "  ...Batch 5000\tEpoch 1\tLoss: 0.12264296\tTrain Acc: 0.95043438\n",
      "  ...Batch 6000\tEpoch 1\tLoss: 0.11498404\tTrain Acc: 0.95361198\n",
      "  ...Batch 7000\tEpoch 1\tLoss: 0.10902813\tTrain Acc: 0.95597433\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  NormalUser     0.9674    0.9865    0.9768      5025\n",
      "   TrollUser     0.9861    0.9664    0.9761      4975\n",
      "\n",
      "    accuracy                         0.9765     10000\n",
      "   macro avg     0.9767    0.9764    0.9765     10000\n",
      "weighted avg     0.9767    0.9765    0.9765     10000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " *** CV Accuracy Improved: 0.0000 -> 0.9765 ***\n",
      "\n",
      "Epoch 1\tLoss: 0.10587170\tTrain Acc: 0.95711951\tCV Acc: 0.97649997\tNo Improvement: 0\n",
      "  ...Batch 1000\tEpoch 2\tLoss: 0.06564424\tTrain Acc: 0.97422656\n",
      "  ...Batch 2000\tEpoch 2\tLoss: 0.06467975\tTrain Acc: 0.97421875\n",
      "  ...Batch 3000\tEpoch 2\tLoss: 0.06437133\tTrain Acc: 0.97409375\n",
      "  ...Batch 4000\tEpoch 2\tLoss: 0.06342972\tTrain Acc: 0.97439844\n",
      "  ...Batch 5000\tEpoch 2\tLoss: 0.06262493\tTrain Acc: 0.97469375\n",
      "  ...Batch 6000\tEpoch 2\tLoss: 0.06171897\tTrain Acc: 0.97504167\n",
      "  ...Batch 7000\tEpoch 2\tLoss: 0.06088700\tTrain Acc: 0.97540625\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  NormalUser     0.9716    0.9871    0.9793      5025\n",
      "   TrollUser     0.9867    0.9709    0.9787      4975\n",
      "\n",
      "    accuracy                         0.9790     10000\n",
      "   macro avg     0.9792    0.9790    0.9790     10000\n",
      "weighted avg     0.9791    0.9790    0.9790     10000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " *** CV Accuracy Improved: 0.9765 -> 0.9790 ***\n",
      "\n",
      "Epoch 2\tLoss: 0.06056293\tTrain Acc: 0.97539221\tCV Acc: 0.97899997\tNo Improvement: 0\n",
      "  ...Batch 1000\tEpoch 3\tLoss: 0.05184475\tTrain Acc: 0.98019531\n",
      "  ...Batch 2000\tEpoch 3\tLoss: 0.05188809\tTrain Acc: 0.97952734\n",
      "  ...Batch 3000\tEpoch 3\tLoss: 0.05182319\tTrain Acc: 0.97914062\n",
      "  ...Batch 4000\tEpoch 3\tLoss: 0.05175149\tTrain Acc: 0.97913281\n",
      "  ...Batch 5000\tEpoch 3\tLoss: 0.05146091\tTrain Acc: 0.97925469\n",
      "  ...Batch 6000\tEpoch 3\tLoss: 0.05096199\tTrain Acc: 0.97942057\n",
      "  ...Batch 7000\tEpoch 3\tLoss: 0.05054084\tTrain Acc: 0.97957031\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  NormalUser     0.9728    0.9883    0.9805      5025\n",
      "   TrollUser     0.9879    0.9721    0.9799      4975\n",
      "\n",
      "    accuracy                         0.9802     10000\n",
      "   macro avg     0.9804    0.9802    0.9802     10000\n",
      "weighted avg     0.9803    0.9802    0.9802     10000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " *** CV Accuracy Improved: 0.9790 -> 0.9802 ***\n",
      "\n",
      "Epoch 3\tLoss: 0.05041379\tTrain Acc: 0.97949180\tCV Acc: 0.98019999\tNo Improvement: 0\n",
      "  ...Batch 1000\tEpoch 4\tLoss: 0.04534744\tTrain Acc: 0.98256250\n",
      "  ...Batch 2000\tEpoch 4\tLoss: 0.04579392\tTrain Acc: 0.98191016\n",
      "  ...Batch 3000\tEpoch 4\tLoss: 0.04600116\tTrain Acc: 0.98147135\n",
      "  ...Batch 4000\tEpoch 4\tLoss: 0.04587263\tTrain Acc: 0.98142773\n",
      "  ...Batch 5000\tEpoch 4\tLoss: 0.04563396\tTrain Acc: 0.98153594\n",
      "  ...Batch 6000\tEpoch 4\tLoss: 0.04525845\tTrain Acc: 0.98169271\n",
      "  ...Batch 7000\tEpoch 4\tLoss: 0.04484770\tTrain Acc: 0.98185379\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  NormalUser     0.9752    0.9879    0.9815      5025\n",
      "   TrollUser     0.9876    0.9747    0.9811      4975\n",
      "\n",
      "    accuracy                         0.9813     10000\n",
      "   macro avg     0.9814    0.9813    0.9813     10000\n",
      "weighted avg     0.9814    0.9813    0.9813     10000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " *** CV Accuracy Improved: 0.9802 -> 0.9813 ***\n",
      "\n",
      "Epoch 4\tLoss: 0.04473453\tTrain Acc: 0.98179158\tCV Acc: 0.98130000\tNo Improvement: 0\n",
      "  ...Batch 1000\tEpoch 5\tLoss: 0.04070863\tTrain Acc: 0.98437500\n",
      "  ...Batch 2000\tEpoch 5\tLoss: 0.04087561\tTrain Acc: 0.98369141\n",
      "  ...Batch 3000\tEpoch 5\tLoss: 0.04096792\tTrain Acc: 0.98349740\n",
      "  ...Batch 4000\tEpoch 5\tLoss: 0.04103985\tTrain Acc: 0.98337305\n",
      "  ...Batch 5000\tEpoch 5\tLoss: 0.04098727\tTrain Acc: 0.98341563\n",
      "  ...Batch 6000\tEpoch 5\tLoss: 0.04069732\tTrain Acc: 0.98355599\n",
      "  ...Batch 7000\tEpoch 5\tLoss: 0.04044576\tTrain Acc: 0.98370201\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  NormalUser     0.9774    0.9877    0.9825      5025\n",
      "   TrollUser     0.9874    0.9769    0.9821      4975\n",
      "\n",
      "    accuracy                         0.9823     10000\n",
      "   macro avg     0.9824    0.9823    0.9823     10000\n",
      "weighted avg     0.9824    0.9823    0.9823     10000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " *** CV Accuracy Improved: 0.9813 -> 0.9823 ***\n",
      "\n",
      "Epoch 5\tLoss: 0.04040142\tTrain Acc: 0.98356079\tCV Acc: 0.98229998\tNo Improvement: 0\n",
      "  ...Batch 1000\tEpoch 6\tLoss: 0.03721875\tTrain Acc: 0.98585156\n",
      "  ...Batch 2000\tEpoch 6\tLoss: 0.03739971\tTrain Acc: 0.98528516\n",
      "  ...Batch 3000\tEpoch 6\tLoss: 0.03753470\tTrain Acc: 0.98510156\n",
      "  ...Batch 4000\tEpoch 6\tLoss: 0.03769611\tTrain Acc: 0.98496289\n",
      "  ...Batch 5000\tEpoch 6\tLoss: 0.03757504\tTrain Acc: 0.98499375\n",
      "  ...Batch 6000\tEpoch 6\tLoss: 0.03731292\tTrain Acc: 0.98506380\n",
      "  ...Batch 7000\tEpoch 6\tLoss: 0.03707696\tTrain Acc: 0.98517299\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  NormalUser     0.9756    0.9875    0.9815      5025\n",
      "   TrollUser     0.9872    0.9751    0.9811      4975\n",
      "\n",
      "    accuracy                         0.9813     10000\n",
      "   macro avg     0.9814    0.9813    0.9813     10000\n",
      "weighted avg     0.9814    0.9813    0.9813     10000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 6\tLoss: 0.03706892\tTrain Acc: 0.98510044\tCV Acc: 0.98130000\tNo Improvement: 1\n",
      "  ...Batch 1000\tEpoch 7\tLoss: 0.03331077\tTrain Acc: 0.98768750\n",
      "  ...Batch 2000\tEpoch 7\tLoss: 0.03369404\tTrain Acc: 0.98696875\n",
      "  ...Batch 3000\tEpoch 7\tLoss: 0.03345120\tTrain Acc: 0.98680990\n",
      "  ...Batch 4000\tEpoch 7\tLoss: 0.03332869\tTrain Acc: 0.98676367\n",
      "  ...Batch 5000\tEpoch 7\tLoss: 0.03300147\tTrain Acc: 0.98683750\n",
      "  ...Batch 6000\tEpoch 7\tLoss: 0.03248394\tTrain Acc: 0.98703385\n",
      "  ...Batch 7000\tEpoch 7\tLoss: 0.03191278\tTrain Acc: 0.98724665\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  NormalUser     0.9751    0.9906    0.9828      5025\n",
      "   TrollUser     0.9904    0.9745    0.9824      4975\n",
      "\n",
      "    accuracy                         0.9826     10000\n",
      "   macro avg     0.9828    0.9826    0.9826     10000\n",
      "weighted avg     0.9827    0.9826    0.9826     10000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " *** CV Accuracy Improved: 0.9823 -> 0.9826 ***\n",
      "\n",
      "Epoch 7\tLoss: 0.03168998\tTrain Acc: 0.98721248\tCV Acc: 0.98259997\tNo Improvement: 0\n",
      "  ...Batch 1000\tEpoch 8\tLoss: 0.03181354\tTrain Acc: 0.98818750\n",
      "  ...Batch 2000\tEpoch 8\tLoss: 0.03216564\tTrain Acc: 0.98753906\n",
      "  ...Batch 3000\tEpoch 8\tLoss: 0.03214927\tTrain Acc: 0.98727083\n",
      "  ...Batch 4000\tEpoch 8\tLoss: 0.03200224\tTrain Acc: 0.98727539\n",
      "  ...Batch 5000\tEpoch 8\tLoss: 0.03172930\tTrain Acc: 0.98737500\n",
      "  ...Batch 6000\tEpoch 8\tLoss: 0.03127531\tTrain Acc: 0.98760938\n",
      "  ...Batch 7000\tEpoch 8\tLoss: 0.03082858\tTrain Acc: 0.98774888\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  NormalUser     0.9757    0.9891    0.9823      5025\n",
      "   TrollUser     0.9888    0.9751    0.9819      4975\n",
      "\n",
      "    accuracy                         0.9821     10000\n",
      "   macro avg     0.9822    0.9821    0.9821     10000\n",
      "weighted avg     0.9822    0.9821    0.9821     10000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 8\tLoss: 0.03062523\tTrain Acc: 0.98771141\tCV Acc: 0.98210001\tNo Improvement: 1\n",
      "  ...Batch 1000\tEpoch 9\tLoss: 0.03109107\tTrain Acc: 0.98845313\n",
      "  ...Batch 2000\tEpoch 9\tLoss: 0.03134260\tTrain Acc: 0.98791797\n",
      "  ...Batch 3000\tEpoch 9\tLoss: 0.03132483\tTrain Acc: 0.98770313\n",
      "  ...Batch 4000\tEpoch 9\tLoss: 0.03125305\tTrain Acc: 0.98759766\n",
      "  ...Batch 5000\tEpoch 9\tLoss: 0.03116010\tTrain Acc: 0.98759219\n",
      "  ...Batch 6000\tEpoch 9\tLoss: 0.03078218\tTrain Acc: 0.98776953\n",
      "  ...Batch 7000\tEpoch 9\tLoss: 0.03029538\tTrain Acc: 0.98791295\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  NormalUser     0.9757    0.9889    0.9822      5025\n",
      "   TrollUser     0.9886    0.9751    0.9818      4975\n",
      "\n",
      "    accuracy                         0.9820     10000\n",
      "   macro avg     0.9821    0.9820    0.9820     10000\n",
      "weighted avg     0.9821    0.9820    0.9820     10000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 9\tLoss: 0.03013368\tTrain Acc: 0.98788282\tCV Acc: 0.98199999\tNo Improvement: 2\n",
      "  ...Batch 1000\tEpoch 10\tLoss: 0.03039299\tTrain Acc: 0.98871875\n",
      "  ...Batch 2000\tEpoch 10\tLoss: 0.03107757\tTrain Acc: 0.98799219\n",
      "  ...Batch 3000\tEpoch 10\tLoss: 0.03096489\tTrain Acc: 0.98780469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ...Batch 4000\tEpoch 10\tLoss: 0.03084052\tTrain Acc: 0.98778906\n",
      "  ...Batch 5000\tEpoch 10\tLoss: 0.03055604\tTrain Acc: 0.98793750\n",
      "  ...Batch 6000\tEpoch 10\tLoss: 0.03020398\tTrain Acc: 0.98809375\n",
      "  ...Batch 7000\tEpoch 10\tLoss: 0.02980291\tTrain Acc: 0.98820536\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  NormalUser     0.9753    0.9881    0.9816      5025\n",
      "   TrollUser     0.9878    0.9747    0.9812      4975\n",
      "\n",
      "    accuracy                         0.9814     10000\n",
      "   macro avg     0.9815    0.9814    0.9814     10000\n",
      "weighted avg     0.9815    0.9814    0.9814     10000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 10\tLoss: 0.02963515\tTrain Acc: 0.98816442\tCV Acc: 0.98140001\tNo Improvement: 3\n",
      "  ...Batch 1000\tEpoch 11\tLoss: 0.02967092\tTrain Acc: 0.98935156\n",
      "  ...Batch 2000\tEpoch 11\tLoss: 0.03039147\tTrain Acc: 0.98847656\n",
      "  ...Batch 3000\tEpoch 11\tLoss: 0.03029736\tTrain Acc: 0.98829427\n",
      "  ...Batch 4000\tEpoch 11\tLoss: 0.03019950\tTrain Acc: 0.98818750\n",
      "  ...Batch 5000\tEpoch 11\tLoss: 0.02993397\tTrain Acc: 0.98825625\n",
      "  ...Batch 6000\tEpoch 11\tLoss: 0.02956669\tTrain Acc: 0.98839714\n",
      "  ...Batch 7000\tEpoch 11\tLoss: 0.02913201\tTrain Acc: 0.98853013\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  NormalUser     0.9756    0.9879    0.9817      5025\n",
      "   TrollUser     0.9876    0.9751    0.9813      4975\n",
      "\n",
      "    accuracy                         0.9815     10000\n",
      "   macro avg     0.9816    0.9815    0.9815     10000\n",
      "weighted avg     0.9816    0.9815    0.9815     10000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 11\tLoss: 0.02905046\tTrain Acc: 0.98842358\tCV Acc: 0.98150003\tNo Improvement: 4\n",
      "  ...Batch 1000\tEpoch 12\tLoss: 0.02908599\tTrain Acc: 0.98940625\n",
      "  ...Batch 2000\tEpoch 12\tLoss: 0.02969026\tTrain Acc: 0.98860547\n",
      "  ...Batch 3000\tEpoch 12\tLoss: 0.02963060\tTrain Acc: 0.98834896\n",
      "  ...Batch 4000\tEpoch 12\tLoss: 0.02937345\tTrain Acc: 0.98833398\n",
      "  ...Batch 5000\tEpoch 12\tLoss: 0.02917323\tTrain Acc: 0.98834687\n",
      "  ...Batch 6000\tEpoch 12\tLoss: 0.02867536\tTrain Acc: 0.98853776\n",
      "  ...Batch 7000\tEpoch 12\tLoss: 0.02826527\tTrain Acc: 0.98870313\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  NormalUser     0.9762    0.9891    0.9826      5025\n",
      "   TrollUser     0.9888    0.9757    0.9822      4975\n",
      "\n",
      "    accuracy                         0.9824     10000\n",
      "   macro avg     0.9825    0.9824    0.9824     10000\n",
      "weighted avg     0.9825    0.9824    0.9824     10000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  NormalUser     0.9766    0.9881    0.9823      4976\n",
      "   TrollUser     0.9881    0.9765    0.9823      5024\n",
      "\n",
      "    accuracy                         0.9823     10000\n",
      "   macro avg     0.9823    0.9823    0.9823     10000\n",
      "weighted avg     0.9824    0.9823    0.9823     10000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Final Test Acc: 0.98229998\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 100\n",
    "class_labels = ['NormalUser', 'TrollUser']\n",
    "lin_model, losses = train_model(x_text, x_pos, x_ent, x_hashtag, x_feats, y_bin, class_labels, lin_model, \n",
    "                optimizer, scheduler, batch_size, epochs, device)\n",
    "#5 classes 0.9113\n",
    "#bin 0.9818"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "stupid-northwest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "annoying-poverty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearClassifier(\n",
       "  (text_embedder): EmbeddingBag(47879, 128, mode=mean)\n",
       "  (pos_embedder): EmbeddingBag(52, 8, mode=mean)\n",
       "  (ent_embedder): EmbeddingBag(19, 8, mode=mean)\n",
       "  (hashtag_embedder): EmbeddingBag(3934, 64, mode=mean)\n",
       "  (feat_transform): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (hid_layer1): Linear(in_features=212, out_features=512, bias=True)\n",
       "  (hid_layer2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (out_layer): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (batchnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_model.load_state_dict(torch.load(\"lin_model_bin.pt\"))\n",
    "lin_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "filled-contest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  NormalUser     0.9747    0.9891    0.9818      4976\n",
      "   TrollUser     0.9891    0.9745    0.9818      5024\n",
      "\n",
      "    accuracy                         0.9818     10000\n",
      "   macro avg     0.9819    0.9818    0.9818     10000\n",
      "weighted avg     0.9819    0.9818    0.9818     10000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Final Test Acc: 0.98180002\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_acc = test(x_text[:10000], x_pos[:10000], x_ent[:10000], x_hashtag[:10000], x_feats[:10000], y_bin[:10000], \n",
    "                lin_model, class_labels, print_confusion=True)\n",
    "\n",
    "print('\\n\\n\\nFinal Test Acc: %.8f' % (test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-trauma",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-overview",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-ordinance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-narrative",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributed embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-drinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_w2v(texts, model_path, vec_dim=128, window=5):\n",
    "    model = Word2Vec(sentences=texts, size=vec_dim, window=window, min_count=10, workers=4)\n",
    "    model.train(texts, total_examples=len(texts), epochs=5)\n",
    "\n",
    "    print('...Storing trained word2vec model')\n",
    "    model.save(model_path)\n",
    "    \n",
    "    for word in ['one', 'cat', 'fun', 'stupid', 'lol']:\n",
    "        print('close to %s' % word)\n",
    "        for w,s in model.wv.most_similar(word):\n",
    "            print('   %s, %.6f' % (w,s))\n",
    "            \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_ft(texts, model_path, vec_dim=128, window=5):\n",
    "    model = FastText(sentences=texts, size=vec_dim, window=window, min_count=10, workers=4)\n",
    "    model.train(texts, total_examples=len(texts), epochs=5)\n",
    "\n",
    "    print('...Storing trained fasttext model')\n",
    "    model.save(model_path)\n",
    "    \n",
    "    for word in ['one', 'cat', 'fun', 'stupid', 'lol']:\n",
    "        print('close to %s' % word)\n",
    "        for w,s in model.wv.most_similar(word):\n",
    "            print('   %s, %.6f' % (w,s))\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-customer",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_texts = [[t for t in l.split(' ') if t and not t[0]=='#'] for l in lemmas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-asset",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v_lemma_128_model = train_w2v(lemma_texts, model_path='w2v_lemmas_128.model', vec_dim=128, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-brick",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ft_lemmas_128_model = train_ft(lemma_texts, model_path='ft_lemmas_128.model', vec_dim=128, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi feature model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, hid1_size, hid2_size, out_size, text_vocab_size, text_emb_dim, \n",
    "                 pos_vocab_size, pos_emb_dim, ent_vocab_size, ent_emb_dim, \n",
    "                 hashtag_vocab_size, hashtag_emb_dim, in_feat_dim, out_feat_dim):\n",
    "        super().__init__()\n",
    "        self.text_embedder = nn.EmbeddingBag(text_vocab_size, text_emb_dim)\n",
    "        self.pos_embedder = nn.EmbeddingBag(pos_vocab_size, pos_emb_dim)\n",
    "        self.ent_embedder = nn.EmbeddingBag(ent_vocab_size, ent_emb_dim)\n",
    "        self.hashtag_embedder = nn.EmbeddingBag(hashtag_vocab_size, hashtag_emb_dim)\n",
    "        self.feat_transform = nn.Linear(in_feat_dim, out_feat_dim)\n",
    "        \n",
    "        self.hid_layer1 = nn.Linear(text_emb_dim + pos_emb_dim + ent_emb_dim + hashtag_emb_dim + out_feat_dim, hid1_size)\n",
    "        self.hid_layer2 = nn.Linear(hid1_size, hid2_size)\n",
    "        self.out_layer = nn.Linear(hid2_size, out_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hid1_size)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hid2_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x_text, x_pos, x_ent, x_hashtag, x_feats):\n",
    "        text_emb = self.text_embedder(x_text)\n",
    "        pos_emb = self.pos_embedder(x_pos)\n",
    "        ent_emb = self.ent_embedder(x_ent)\n",
    "        hashtag_emb = self.hashtag_embedder(x_hashtag)\n",
    "        feat_trans = self.feat_transform(x_feats)\n",
    "        \n",
    "        x = torch.cat((text_emb, pos_emb, ent_emb, hashtag_emb, feat_trans), dim=1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.hid_layer1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        \n",
    "        x = self.relu(self.hid_layer2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        \n",
    "        x = self.out_layer(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index mappings for embedding types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature2idx_dicts(feats, word_model):\n",
    "    word2idx = {}\n",
    "    for i in range(len(word_model.wv.vocab)):\n",
    "        word2idx[word_model.wv.index2word[i]] = i\n",
    "\n",
    "    pos2idx = {}\n",
    "    i=1\n",
    "    for feat in feats:\n",
    "        for pos in feat['pos'].split(' '):\n",
    "            if not pos:\n",
    "                continue\n",
    "            if pos not in pos2idx:\n",
    "                pos2idx[pos] = i\n",
    "                i+=1\n",
    "    pos2idx['<PAD>'] = 0\n",
    "\n",
    "    ent2idx = {}\n",
    "    i=1\n",
    "    for feat in feats:\n",
    "        for et in feat['ent_types']:\n",
    "            #skip some bogus entity types\n",
    "            if not et or not et.isupper():\n",
    "                continue\n",
    "            if et not in ent2idx:\n",
    "                ent2idx[et] = i\n",
    "                i+=1\n",
    "    ent2idx['<PAD>'] = 0\n",
    "\n",
    "    #only use hashtags with over 10 occurrences\n",
    "    hashtags = []\n",
    "    for feat in feats:\n",
    "        hashtags.extend(feat['hashtags'])\n",
    "\n",
    "    hashtag_cts = Counter(hashtags)\n",
    "    hashtag_cts = {h:c for h,c in hashtag_cts.items() if c>=10}\n",
    "\n",
    "    hashtag2idx = {h:i+1 for i,h in enumerate(hashtag_cts.keys())}\n",
    "    hashtag2idx['<PAD>'] = 0\n",
    "\n",
    "    feat_fields = ['emoji_ratio', 'link_ratio', 'user_ratio', 'oov_ratio']\n",
    "    \n",
    "    return word2idx, pos2idx, ent2idx, hashtag2idx, feat_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feature2idx_dicts(w2v_lemma_128_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-excitement",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "for i in range(len(w2v_lemma_128_model.wv.vocab)):\n",
    "    word2idx[w2v_lemma_128_model.wv.index2word[i]] = i\n",
    "#word2idx['<PAD>'] = 0\n",
    "idx2word = {i:w for w,i in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-claim",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos2idx = {}\n",
    "i=1\n",
    "for feat in feats:\n",
    "    for pos in feat['pos'].split(' '):\n",
    "        if not pos:\n",
    "            continue\n",
    "        if pos not in pos2idx:\n",
    "            pos2idx[pos] = i\n",
    "            i+=1\n",
    "pos2idx['<PAD>'] = 0\n",
    "pos2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-saskatchewan",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ent2idx = {}\n",
    "i=1\n",
    "for feat in feats:\n",
    "    for et in feat['ent_types']:\n",
    "        if not et or not et.isupper():\n",
    "            continue\n",
    "        if et not in ent2idx:\n",
    "            ent2idx[et] = i\n",
    "            i+=1\n",
    "ent2idx['<PAD>'] = 0\n",
    "ent2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = []\n",
    "for feat in feats:\n",
    "    hashtags.extend(feat['hashtags'])\n",
    "    \n",
    "hashtag_cts = Counter(hashtags)\n",
    "hashtag_cts = {h:c for h,c in hashtag_cts.items() if c>=10}\n",
    "\n",
    "hashtag2idx = {h:i+1 for i,h in enumerate(hashtag_cts.keys())}\n",
    "hashtag2idx['<PAD>'] = 0\n",
    "\n",
    "len(hashtag_cts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_fields = ['emoji_ratio', 'link_ratio', 'user_ratio', 'oov_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = ['NewsFeed', 'NormalUser', 'RightTroll', 'LeftTroll', 'HashtagGamer']\n",
    "#class_labels = ['NormalUser', 'TrollUser']\n",
    "lab2idx = {l:i for i,l in enumerate(sorted(class_labels))}\n",
    "idx2lab = {i:l for l,i in lab2idx.items()}\n",
    "lab2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = LinearClassifier(hid1_size=512, hid2_size=128, out_size=5, \n",
    "                             text_vocab_size=len(word2idx), text_emb_dim=w2v_lemma_128_model.wv.vector_size, \n",
    "                             pos_vocab_size=len(pos2idx), pos_emb_dim=8,\n",
    "                             ent_vocab_size=len(ent2idx), ent_emb_dim=8, \n",
    "                             hashtag_vocab_size=len(hashtag2idx), hashtag_emb_dim=64, \n",
    "                             in_feat_dim=len(feat_fields), out_feat_dim=len(lab2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model.text_embedder.weight.data.copy_(torch.FloatTensor(w2v_lemma_128_model.wv.vectors))\n",
    "lin_model.text_embedder.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "lin_model.to(device)\n",
    "lin_model.train()\n",
    "\n",
    "optimizer = optim.Adam(lin_model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 30\n",
    "ent_seq_len = 2\n",
    "hashtag_seq_len = 2\n",
    "x_text = []\n",
    "x_pos = []\n",
    "x_ent = []\n",
    "x_hashtag = []\n",
    "x_feats = []\n",
    "y = []\n",
    "for feat in feats:\n",
    "    if feat['type'] not in lab2idx:\n",
    "        continue\n",
    "        \n",
    "    s = [word2idx[t] for t in feat['lemmas'].split(' ') if t and t in word2idx] + [word2idx['pad']]*seq_len\n",
    "    x_text.append(torch.tensor(s[:seq_len]))\n",
    "    \n",
    "    s = [pos2idx[t] for t in feat['pos'].split(' ') if t and t in pos2idx] + [pos2idx['<PAD>']]*seq_len\n",
    "    x_pos.append(torch.tensor(s[:seq_len]))\n",
    "    \n",
    "    s = [ent2idx[t] for t in feat['ent_types'] if t and t in ent2idx] + [ent2idx['<PAD>']]*ent_seq_len\n",
    "    x_ent.append(torch.tensor(s[:ent_seq_len]))\n",
    "    \n",
    "    s = [hashtag2idx[t] for t in feat['hashtags'] if t and t in hashtag2idx] + [hashtag2idx['<PAD>']]*hashtag_seq_len\n",
    "    x_hashtag.append(torch.tensor(s[:hashtag_seq_len]))\n",
    "    \n",
    "    x_feats.append(torch.tensor([feat[f] for f in feat_fields]))\n",
    "    \n",
    "    y.append(lab2idx[feat['type']])\n",
    "    \n",
    "x_text = torch.vstack(x_text)\n",
    "x_pos = torch.vstack(x_pos)\n",
    "x_ent = torch.vstack(x_ent)\n",
    "x_hashtag = torch.vstack(x_hashtag)\n",
    "x_feats = torch.vstack(x_feats)\n",
    "y = torch.tensor(y)\n",
    "\n",
    "torch.save(x_text, 'x_text.pt')\n",
    "torch.save(x_pos, 'x_pos.pt')\n",
    "torch.save(x_ent, 'x_ent.pt')\n",
    "torch.save(x_hashtag, 'x_hashtag.pt')\n",
    "torch.save(x_feats, 'x_feats.pt')\n",
    "torch.save(y, 'y.pt')\n",
    "\n",
    "x_text.size(), x_pos.size(), x_ent.size(), x_hashtag.size(), x_feats.size(), y.size(), x_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.load('y.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = ['NewsFeed', 'NormalUser', 'RightTroll', 'LeftTroll', 'HashtagGamer']\n",
    "y_bin = []\n",
    "for feat in feats:\n",
    "    if feat['type'] not in labs:\n",
    "        continue\n",
    "    y_bin.append(0 if feat['type']=='NormalUser' else 1)\n",
    "len(y_bin), Counter(y_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bin = torch.tensor(y_bin)\n",
    "torch.save(y_bin, 'y_bin.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_text, x_pos, x_ent, x_hashtag, x_feats, y, class_labels, model, \n",
    "                optimizer, scheduler, criterion, batch_size, epochs, device):\n",
    "    x_text_test = x_text[:10000]\n",
    "    x_text_cv = x_text[10000:20000]\n",
    "    x_text_train = x_text[20000:]\n",
    "\n",
    "    x_pos_test = x_pos[:10000]\n",
    "    x_pos_cv = x_pos[10000:20000]\n",
    "    x_pos_train = x_pos[20000:]\n",
    "\n",
    "    x_ent_test = x_ent[:10000]\n",
    "    x_ent_cv = x_ent[10000:20000]\n",
    "    x_ent_train = x_ent[20000:]\n",
    "\n",
    "    x_hashtag_test = x_hashtag[:10000]\n",
    "    x_hashtag_cv = x_hashtag[10000:20000]\n",
    "    x_hashtag_train = x_hashtag[20000:]\n",
    "\n",
    "    x_feats_test = x_feats[:10000]\n",
    "    x_feats_cv = x_feats[10000:20000]\n",
    "    x_feats_train = x_feats[20000:]\n",
    "\n",
    "    y_test = y[:10000]\n",
    "    y_cv = y[10000:20000]\n",
    "    y_train = y[20000:]\n",
    "    \n",
    "    x_text_train_batches = [x_text_train[i*batch_size:(i+1)*batch_size] for i in range((x_text_train.shape[0]//batch_size) + 1)]\n",
    "    x_pos_train_batches = [x_pos_train[i*batch_size:(i+1)*batch_size] for i in range((x_pos_train.shape[0]//batch_size) + 1)]\n",
    "    x_ent_train_batches = [x_ent_train[i*batch_size:(i+1)*batch_size] for i in range((x_ent_train.shape[0]//batch_size) + 1)]\n",
    "    x_hashtag_train_batches = [x_hashtag_train[i*batch_size:(i+1)*batch_size] for i in range((x_hashtag_train.shape[0]//batch_size) + 1)]\n",
    "    x_feats_train_batches = [x_feats_train[i*batch_size:(i+1)*batch_size] for i in range((x_feats_train.shape[0]//batch_size) + 1)]\n",
    "    y_train_batches = [y_train[i*batch_size:(i+1)*batch_size] for i in range((y_train.shape[0]//batch_size) + 1)]\n",
    "    \n",
    "    best_cv_acc = 0\n",
    "    losses = []\n",
    "    no_improvement = 0\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        \n",
    "        #idx = list(range(len(x_text_train_batches)))\n",
    "        #random.shuffle(idx)\n",
    "        #x_batches = [x_batches[i] for i in idx]\n",
    "        #y_batches = [y_batches[i] for i in idx]\n",
    "        \n",
    "        for i in range(len(x_text_train_batches)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(x_text_train_batches[i].to(device),\n",
    "                           x_pos_train_batches[i].to(device), \n",
    "                           x_ent_train_batches[i].to(device),\n",
    "                           x_hashtag_train_batches[i].to(device), \n",
    "                           x_feats_train_batches[i].to(device))\n",
    "\n",
    "            #y_pred = torch.argmax(y_pred, dim=1).float().unsqueeze(1)\n",
    "            #loss = criterion(y_pred, y_train_batches[i])#)\n",
    "            loss = F.nll_loss(y_pred, y_train_batches[i])\n",
    "            acc = binary_acc(y_pred, y_train_batches[i].float())#.unsqueeze(1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            if i and i%1000==0:\n",
    "                print('  ...Batch %d\\tEpoch %d\\tLoss: %.8f\\tTrain Acc: %.8f' % (\n",
    "                    i, epoch, epoch_loss/i, epoch_acc/i))\n",
    "                losses.append(epoch_loss/i)\n",
    "        \n",
    "        cv_acc = test(x_text_cv, x_pos_cv, x_ent_cv, x_hashtag_cv, x_feats_cv, y_cv, \n",
    "                      model, class_labels)\n",
    "        if cv_acc>best_cv_acc:\n",
    "            print('\\n *** CV Accuracy Improved: %.4f -> %.4f ***' % (best_cv_acc, cv_acc))\n",
    "            torch.save(model.state_dict(), 'lin_model_bin.pt')\n",
    "            best_cv_acc=cv_acc\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "        \n",
    "        if no_improvement >= 10:\n",
    "            break\n",
    "            \n",
    "        scheduler.step(cv_acc)\n",
    "            \n",
    "        print('\\nEpoch %d\\tLoss: %.8f\\tTrain Acc: %.8f\\tCV Acc: %.8f\\tNo Improvement: %d' % (\n",
    "            epoch, epoch_loss/len(x_text_train_batches), epoch_acc/len(x_text_train_batches), cv_acc, no_improvement))\n",
    "        \n",
    "    test_acc = test(x_text_test, x_pos_test, x_ent_test, x_hashtag_test, x_feats_test, y_test, \n",
    "                    model, class_labels, print_confusion=True)\n",
    "    print('\\n\\n\\nFinal Test Acc: %.8f' % (test_acc))\n",
    "        \n",
    "    return model, losses\n",
    "        \n",
    "        \n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred = torch.argmax(y_pred, dim=1).cpu()\n",
    "    cor = (y_pred == y_test).int().sum()\n",
    "    acc = cor/y_test.size()[0]\n",
    "    \n",
    "    return acc\n",
    "\n",
    "\n",
    "def test(x_text_test, x_pos_test, x_ent_test, x_hashtag_test, x_feats_test, y_test, \n",
    "         model, class_labels, print_confusion=False):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x_text_test, x_pos_test, x_ent_test, x_hashtag_test, x_feats_test)\n",
    "        y_pred = torch.argmax(y_pred, dim=1).cpu()\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    print(classification_report(y_test.tolist(), y_pred.tolist(), target_names=class_labels, digits=4))\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    if print_confusion:\n",
    "        cm = confusion_matrix(y_test.tolist(), y_pred.tolist())\n",
    "        f = sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_labels, yticklabels=class_labels)\n",
    "\n",
    "    cor = (y_pred == y_test).int().sum()\n",
    "    acc = cor/y_test.size()[0]\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-oriental",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 100\n",
    "class_labels = ['NormalUser', 'TrollUser']\n",
    "lin_model, losses = train_model(x_text, x_pos, x_ent, x_hashtag, x_feats, y_bin, class_labels, lin_model, \n",
    "                optimizer, scheduler, criterion, batch_size, epochs, device)\n",
    "#0.9113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-trademark",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-bidder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-cornell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model.load_state_dict(torch.load(\"lin_model.pt\"))\n",
    "lin_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test(x_text[:10000], x_pos[:10000], x_ent[:10000], x_hashtag[:10000], x_feats[:10000], y[:10000], \n",
    "                lin_model, class_labels, print_confusion=True)\n",
    "\n",
    "print('\\n\\n\\nFinal Test Acc: %.8f' % (test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-vehicle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-richardson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary accuracy\n",
    "lin_model.eval()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    y_pred = lin_model(x_text_test, x_pos_test, x_ent_test, x_hashtag_test, x_feats_test)\n",
    "    y_pred = torch.argmax(y_pred, dim=1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred.size()\n",
    "ys= [0 if y==3 else 1 for y in y_pred.tolist()]\n",
    "Counter(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\n')\n",
    "print(classification_report(y_test.tolist(), y_pred.tolist(), target_names=class_labels, digits=4))\n",
    "print('\\n\\n')\n",
    "\n",
    "if print_confusion:\n",
    "    cm = confusion_matrix(y_test.tolist(), y_pred.tolist())\n",
    "    f = sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_labels, yticklabels=class_labels)\n",
    "\n",
    "cor = (y_pred == y_test).int().sum()\n",
    "acc = cor/y_test.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "#you need to explicitly set the path for this lib\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = lin_model(x_text[:10], x_pos[:10], x_ent[:10], x_hashtag[:10], x_feats[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dot(out, params=dict(list(lin_model.named_parameters()))).render(\"linear_model\", format=\"jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum tfidf weighted word vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-australia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_tfidf_weighted_word_vectors(texts, word_model, tfidf, tfidf_vocab):\n",
    "    sum_vecs = []\n",
    "    eps = 0.01\n",
    "    \n",
    "    for i in range(len(texts)):\n",
    "        wts = []\n",
    "        vecs = []\n",
    "        \n",
    "        for tk in texts[i].split(' '):\n",
    "            if tk not in word_model.wv:\n",
    "                continue\n",
    "\n",
    "            vecs.append(word_model.wv[tk])\n",
    "\n",
    "            if tk not in tfidf_vocab:\n",
    "                wts.append(eps)\n",
    "            else:\n",
    "                wts.append(tfidf[i, tfidf_vocab.index(tk)])\n",
    "        \n",
    "        wts = np.array(wts)\n",
    "        if not vecs:\n",
    "            #just add random word vec\n",
    "            #idx = random.randint(0,len(word_model.wv.vocab)-1)\n",
    "            #sum_vecs.append(word_model.wv[idx])\n",
    "            sum_vecs.append(np.ones(word_model.wv.vector_size)*0.01)\n",
    "        else:\n",
    "            wt_ttl = np.sum(wts)\n",
    "            wts /= wt_ttl\n",
    "            vecs = np.vstack(vecs)\n",
    "            sum_vecs.append(np.dot(wts, vecs))\n",
    "            #sum_w2v_vecs.append(np.sum([vecs[i]*wts[i] for i in range(len(wts))], axis=0))\n",
    "        #labs.append(labels[i])\n",
    "        \n",
    "        if i and i%1000==0:\n",
    "            print(i)\n",
    "    return sum_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_tfidf_weighted_w2v = get_sum_tfidf_weighted_word_vectors(lemmas[:10000], w2v_lemma_128_model, tfidf_lemmas, lemma_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_tfidf_weighted_w2v_vecs = np.vstack(sum_tfidf_weighted_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-convertible",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-socket",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPOSTagger(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim, \n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 dropout, \n",
    "                 pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        #reference: https://stackoverflow.com/questions/62291303/pytorch-loading-word-vectors-into-field-vocabulary-vs-embedding-layer\n",
    "        # Define an embedding layer that converts the words to embeddings based on GloVe.\n",
    "        #self.embedding = nn.Embedding.from_pretrained(TEXT.vocab.vectors, padding_idx=pad_idx)\n",
    "        self.embedding = nn.Embedding(TEXT.vocab.vectors.size(0), TEXT.vocab.vectors.size(1), padding_idx=pad_idx)\n",
    "        \n",
    "        # Define a bi-directional LSTM layer with the hyperparameters. \n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_dim, num_layers = n_layers, \n",
    "                            batch_first=True, bidirectional=bidirectional)\n",
    "        \n",
    "        # Define a dropout layer that helps in regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Define a Linear layer which can associate lstm output to the final output \n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim) # *2 for bidirectional output from lstm\n",
    "        \n",
    "        \n",
    "    def forward(self, text):\n",
    "        predictions = []\n",
    "    \n",
    "        # pass text through embedding layer (sent len x batch size)\n",
    "        emb = self.embedding(text)\n",
    "        #print('emb', emb.size()) #batch x seq x dim?\n",
    "        \n",
    "        # pass embeddings into LSTM  batch x seq x dim\n",
    "        final_hid, _ = self.lstm(emb) #embeds.view(len(sentence), 1, -1)  \n",
    "        #print('final_hid', final_hid.size()) #(N, L, D * H_{out})  batch x seq x 2*hid\n",
    "        \n",
    "        #print('final hid compressed', )\n",
    "        # pass the LSTM output to dropout and fully connected linear layer\n",
    "        predictions = self.fc(final_hid) #lstm_out.view(len(sentence), -1)  do we compress seq into the batch?\n",
    "        #print('predictions', predictions.size()) #final_hid.view(-1, 2*self.hidden_dim)\n",
    "        \n",
    "        # we use our outputs to make a prediction of what the tag should be\n",
    "        \n",
    "        # predictions = [sent len, batch size, output dim]\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100 #glove dim\n",
    "HIDDEN_DIM = 256 #128\n",
    "OUTPUT_DIM = len(UD_TAGS.vocab)\n",
    "N_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.2\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = LSTMPOSTagger(INPUT_DIM, \n",
    "                        EMBEDDING_DIM, \n",
    "                        HIDDEN_DIM, \n",
    "                        OUTPUT_DIM, \n",
    "                        N_LAYERS, \n",
    "                        BIDIRECTIONAL, \n",
    "                        DROPOUT, \n",
    "                        PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, std=0.1)\n",
    "model.apply(init_weights)\n",
    "\n",
    "# initializing model embeddings with glove word vectors\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "# making the padding embeddings as all zero, as we don't want to learn paddings.\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# optimizer to train the model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# ignoring the padding in our loss calculation\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "# use gpu if available, These lines move your model to gpu from cpu if available\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "no_improvement = 0\n",
    "prev_val_loss = 99999999\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    model.train()\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "    print(f'Epoch: {epoch+1:02}\\n')\n",
    "    num_batches=0\n",
    "    for batch in train_iterator:\n",
    "        num_batches+=1\n",
    "        \n",
    "        # returns a batch of text to train on (sent len, batch size)\n",
    "        text = batch.text\n",
    "        tags = batch.udtags\n",
    "        \n",
    "        # Add a command that makes the optimizer with zero gradients for each iteration \n",
    "         # Add code line\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Add a command that feeds the batch to the model\n",
    "         # Add code line\n",
    "        predictions = model(text)\n",
    "        \n",
    "        # predictions = (sent len, batch size, output dim)\n",
    "          # tags = (sent len, batch size)\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        \n",
    "        # Add a command that calculates loss\n",
    "         # Add code line\n",
    "        train_loss = criterion(predictions, tags)\n",
    "        train_epoch_loss += train_loss.item()\n",
    "        \n",
    "        # Make use of the categorical accuracy and calculate accuracy\n",
    "         # Add code line\n",
    "        train_acc = categorical_accuracy(predictions, tags)\n",
    "        train_epoch_acc += train_acc.item()\n",
    "        \n",
    "        # Add a command that calculates gradients\n",
    "         # Add code line\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # Add a command that updates the weights by taking steps \n",
    "         # Add code line\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    # Calculate loss\n",
    "    print('%d batches' % num_batches)\n",
    "    train_epoch_loss /= num_batches\n",
    "    train_epoch_acc /= num_batches\n",
    "        \n",
    "    print(f'\\t [Train Loss] : {train_epoch_loss:.3f} | [Train Acc] : {train_epoch_acc*100:.2f}%\\n')\n",
    "    \n",
    "    val_epoch_loss = 0\n",
    "    val_epoch_acc = 0\n",
    "    \n",
    "    # Add a command that moves the model to validation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        num_val_batches=0\n",
    "        for batch in valid_iterator:\n",
    "            num_val_batches+=1\n",
    "            \n",
    "            text = batch.text\n",
    "            tags = batch.udtags\n",
    "        \n",
    "            # Add the same command that feeds the batch to the model\n",
    "            # Add code line\n",
    "            predictions = model(text)\n",
    "            \n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            \n",
    "            # Add the same command that calculates loss\n",
    "            # Add code line\n",
    "            val_loss = criterion(predictions, tags)\n",
    "            val_epoch_loss += val_loss.item()\n",
    "            \n",
    "            # Make use of the categorical accuracy function and calculate accuracy\n",
    "             # Add code line\n",
    "            val_acc = categorical_accuracy(predictions, tags)\n",
    "            val_epoch_acc += val_acc.item()\n",
    "            \n",
    "        # Calculate validation loss\n",
    "        print('%d val batches' % num_val_batches)\n",
    "        val_epoch_loss /= num_val_batches\n",
    "        val_epoch_acc /= num_val_batches\n",
    "        \n",
    "        if val_epoch_loss < prev_val_loss:\n",
    "            prev_val_loss = val_epoch_loss\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            if no_improvement >= 10:\n",
    "                print('No improvement, stopping early')\n",
    "                break\n",
    "            \n",
    "        print(f'\\t [Val Loss] : {val_epoch_loss:.3f} | [Val Acc] : {val_epoch_acc*100:.2f}%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-impossible",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-dressing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
