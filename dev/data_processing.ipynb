{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "biblical-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# uncomment any library you need to install and run the cell\n",
    "\n",
    "#!{sys.executable} -m pip install numpy\n",
    "#!{sys.executable} -m pip install matplotlib\n",
    "#!{sys.executable} -m pip install sklearn\n",
    "#!{sys.executable} -m pip install emoji\n",
    "#!{sys.executable} -m pip install gensim\n",
    "\n",
    "#!{sys.executable} -m pip install spacy\n",
    "#!{sys.executable} -m spacy download en_core_web_sm\n",
    "\n",
    "#!{sys.executable} -m pip install nltk\n",
    "#nltk.download('words')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "narrow-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../app')\n",
    "from config import Config\n",
    "from preprocessing import Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "imposed-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "higher-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessing(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-participant",
   "metadata": {},
   "source": [
    "## Run all at once, or process by process\n",
    "### All at once, skips steps if file already exists, otherwise takes a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-winner",
   "metadata": {},
   "source": [
    "### Run process by process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "determined-proxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting raw troll tweets...\n",
      "Extracted 2116866 troll tweets (badtabs: 0, badlines: 1, skipped nonenglish: 829340)\n",
      "Extracting raw user tweets...\n",
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "bad tabs: 217985, bad lines: 223\n",
      "Extracted 8783464 normal user tweets (badtabs: 217985, badlines: 223)\n"
     ]
    }
   ],
   "source": [
    "# extract and aggregate raw data\n",
    "preprocessor.extract_type_and_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intelligent-remainder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 2116866 cleaned troll tweets to ../data/troll_tweets_clean.pkl.gz\n",
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "Saving 8783464 cleaned user tweets to ../data/user_tweets_clean.pkl.gz\n"
     ]
    }
   ],
   "source": [
    "# clean the data\n",
    "preprocessor.clean_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "educational-senegal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing troll tweets tagged so far: 0 commercial\t<LINK> So, did you crush your workout? <USER> @_kaskp <USER> <USER> <USER> <USER> <USER>\t<LINK> So , did you crush your workout ? <USER> @_kaskp <USER> <USER> <USER> <USER> <USER>\t<LINK> so , do you crush your workout ? <USER> @_kaskp <USER> <USER> <USER> <USER> <USER>\tTAG RB , VBD PRP VB PRP$ NN . TAG NNP TAG TAG TAG TAG TAG\tyour_workout\t\n",
      "Storing complete 100000 tagged troll tweets\n",
      "Storing user tweets tagged so far: 0 NormalUser\tCongrats to Toby Warrior Shaye G. of New York, NY, the winner of the Toby Keith Norway Flyaway Sweepstakes! - <LINK>\tCongrats to Toby Warrior Shaye G. of New York , NY , the winner of the Toby Keith Norway Flyaway Sweepstakes ! - <LINK>\tcongrat to Toby Warrior Shaye G. of New York , NY , the winner of the Toby Keith Norway Flyaway sweepstake ! - <LINK>\tNNS IN NNP NNP NNP NNP IN NNP NNP , NNP , DT NN IN DT NNP NNP NNP NNP NNS . : TAG\tToby_Warrior_Shaye_G. New_York the_winner the_Toby_Keith_Norway_Flyaway_Sweepstakes\tToby:ORG New_York:GPE NY:ORG\n",
      "Storing complete 100000 tagged user tweets\n"
     ]
    }
   ],
   "source": [
    "# use spacy NLP to tokenize tweets - takes a very long time\n",
    "preprocessor.tokenize_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and store the feature records\n",
    "preprocessor.get_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-alliance",
   "metadata": {},
   "source": [
    "## Mix and store a subset of the data to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "scientific-movie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('../data/troll_features.pkl.gz', 'rb') as fz:\n",
    "    troll_feats = pickle.load(fz)\n",
    "troll_feats = troll_feats[:500000]\n",
    "len(troll_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "passive-mattress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('../data/user_features.pkl.gz', 'rb') as fz:\n",
    "    user_feats = pickle.load(fz)\n",
    "user_feats = user_feats[:len(troll_feats)]\n",
    "len(user_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "scenic-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(troll_feats)\n",
    "random.shuffle(user_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "martial-scheduling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = troll_feats\n",
    "while user_feats:\n",
    "    feats.append(user_feats.pop())\n",
    "random.shuffle(feats)\n",
    "len(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "combined-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [1 if f['type']=='NormalUser' else 0 for f in feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "confident-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(config.feature_x_path, 'wb') as oz:\n",
    "    pickle.dump(feats, oz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "center-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(config.feature_y_path, 'wb') as oz:\n",
    "    pickle.dump(y, oz)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
